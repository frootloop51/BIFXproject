---
title: 'Final Exam: Building a  Neural Network Model with keras to Predict Breast
  Cancer'
author: "Ian Lacy"
date: "2023-05-10"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}

suppressMessages(library(readr))
suppressMessages(library(keras))
suppressMessages(library(caret))
```


## Introduction

This analysis attempts to design a neural network model that predicts the outcome of cancer screenings.

## Importing the Data

We begin by importing the dataset from the CSV file. This will give us raw, unnormalized data that will require pre-processing before it can be added to keras.

```{r}
CancerFinal <- read_csv("C:/Users/ianla.000/Downloads/data.csv")

```

## Cleaning Data

```{r}
CancerFinal <- CancerFinal[,-33]
#str(CancerFinal)
#summary(CancerFinal)

```

If we were to display the data, we would see that most of the data is numeric except for the target variable diagnosis. For this reason we will need to code a dummy variable to convert this to a numeric value. I began by copying the unaltered data into a second dataframe: CancerFinal2. This is to prevent corrupting the original data.

```{r}
CancerFinal2 <- CancerFinal
table(CancerFinal2$diagnosis)
round(prop.table(table(CancerFinal2$diagnosis)) *100, digits =1)
```
I also generated a table to look at the distribution of the target variables. We can see that the values are unbalanced, so when I go to split the data into test and training sets I will use createDataPartition() to conduct a stratified split of the data to preserve the proportionality of the data.

Next, I coded the dummy variable for the diagnosis column and will copy those values into a new column. The new dummy variables will define malignant,M, as 1 and benign, B, as 0. I will then remove the original column because having it in the middle of the data makes it difficult to remove it from the analysis.

I also removed the columns id and diagnosis to reduce unnecessary/redundant variables.

```{r}
CancerFinal2$NewDiagnosis<- ifelse(CancerFinal2$diagnosis == "M", 0, 1) #recoded R column in new column and set M = 1 and B = 0

CancerFinal2 <- subset(CancerFinal2, select = -c(id,diagnosis)) #used subsetting function to remove the original R column
#str(CancerFinal2)
#CancerFinal2

```

Next, I normalized the data. This is important "...because when you input unnormalised inputs to activation functions, you can get stuck in a very flat region in the domain and may not learn at all." (https://stats.stackexchange.com/questions/458579/should-i-normalize-all-data-prior-feeding-the-neural-network-models#:~:text=Yes%2C%20normalisation%2Fscaling%20is%20typically,may%20not%20learn%20at%20all.)

```{r}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x))) } #normalized data using the technique taught in the lecture slides.
CancerNorm <- CancerFinal2
CancerNorm <- as.data.frame(lapply(CancerNorm, normalize))
#summary(CancerNorm)
```

Now that the data is normalized, I will split it into a test and data set. 

I used the to_categorical function to one hot encode the data. 

```{r}
set.seed(123)
Cancer_train <- createDataPartition(y=CancerNorm$NewDiagnosis, p=0.75, list=FALSE)



x_train <- as.matrix(CancerNorm[Cancer_train, 1:30])
x_test <- as.matrix(CancerNorm[-Cancer_train, 1:30])

y_train <- as.matrix(CancerNorm[Cancer_train, 31])
y_test <- as.matrix(CancerNorm[-Cancer_train, 31])


```


The testing and training data are converted to matrices because that allows keras to read the data properly. 

Now that the data is stored as a matrix. I used an 80/20 split using the createDataParition function to preserve the ratios. I used this source to guide my code: https://olgabelitskaya.github.io/keras_cookbook_r.html 

## Creating Model

Now that the data is cleaned we will create the model. I used this resource and used some of their code segments to structure my model: https://github.com/juanklopper/Deep-learning-using-R/blob/master/Deep%20neural%20network%20example%20using%20R.Rmd. 

I used 30 for the number of nodes and the input shape because we have 30 feature variables in our data. I used sigmoid in the final layer because we are doing binary classification.

Note: The model presented here is not the first model I ran. I had to keep adding layers to improve the loss of the model. The model below represents the best loss, base model.

```{r}

model <- keras_model_sequential()
model %>% 
  layer_dense(name = "DeepLayer1",
              units = 30,
              activation = "relu",
              input_shape = c(30)) %>% 
  layer_dense(name = "DeepLayer2",
              units = 30,
              activation = "relu") %>% 
  layer_dense(name = "DeepLayer3",
              units = 30,
              activation = "relu") %>%
  layer_dense(name = "DeepLayer4",
              units = 30,
              activation = "relu") %>%
  layer_dense(name = "DeepLayer5",
              units = 30,
              activation = "relu") %>%
  layer_dense(name = "DeepLayer6",
              units = 30,
              activation = "relu") %>%
  layer_dense(name = "DeepLayer7",
              units = 30,
              activation = "relu") %>%   
  layer_dense(name = "OutputLayer",
              units = 1,
              activation = "sigmoid")
summary(model)

```
## Compiling the model

Now that our model is set, we can compile the model by specifying the functions for our data.

```{r}
model %>% compile(loss = "binary_crossentropy",
                  optimizer = "rmsprop",
                  metrics = c("accuracy"))
```

I then setup the model and set the epochs at 10, meaning it will run ten times. I set the batch size, the number of samples that propagate through the neural network before updating the model parameters, at 64 (https://hasty.ai/docs/mp-wiki/training-parameters/batch-size#:~:text=The%20batch%20size%20depicts%20the,and%20one%20full%20backward%20propagation). This seemed an appropriate number to allow an adequate number of adjustments without creating too much noise in the data.

```{r}
history <- model %>% 
  fit(x_train,
      y_train,
      epoch = 20,
      batch_size = 128,
      validation_split = 0.1,
      verbose = 2)
```

I also plotted the epochs performance and loss over time to show how the model improves as it iterates over the data.

```{r}
plot(history)
```
```{r}
model %>% 
  evaluate(x_test,
           y_test)
```
```{r}
## prediction 
pred <- model %>% predict(x_test)
y_pred = round(pred)

# Confusion matrix
CM = table(y_pred, y_test)

CM

```
As we can see, the model performs quite well and the computed accuracy is about 92%. But I also generated a confusion matrix (see above) to parse out more details about the data.

```{r}
acc1 <- round((44+58)/(113), digits = 2)
sprintf("Accuracy: %s", acc1)

prec1<-round(44/(44+2), digits =2)
sprintf("Precision: %s", prec1)

rec1 <- round(44/(44+9),digits = 2)
sprintf("Recall: %s", rec1)

f_One1<-round(2*(prec1*rec1)/(prec1+rec1),digits = 2)
sprintf("F-1: %s", f_One1)
```
The above metric outputs confirm the accuracy computed in the previous code chunk.

Precision, proportion of positive identifications, is also high at 96%. In our case, this means that when we predict a tumor is malignant, it is correct 96% of the time.

Recall, proportion of actual positives, was 83%. This means we correctly identify 83% of malignant tumors.

F-1 is also quite high at 89%, indicating that overall this model has strong predictive value.

## Improving the Model

It appears the model is overfit. This is most likely because there are so many parameters (30+). Based on the lecture slides, I will build a smaller model and see if this reduces overfitting. For this, I reduced the number of units to 10, or 1/3rd the parameters compared to the original model.

```{r}
smaller_model <- keras_model_sequential()
smaller_model %>% 
  layer_dense(name = "DeepLayer1",
              units = 10,
              activation = "relu",
              input_shape = c(30)) %>% 
  layer_dense(name = "DeepLayer2",
              units = 10,
              activation = "relu") %>% 
  layer_dense(name = "OutputLayer",
              units = 1,
              activation = "sigmoid")
summary(smaller_model)
```
```{r}
smaller_model %>% compile(loss = "binary_crossentropy",
                  optimizer = "rmsprop",
                  metrics = c("accuracy"))
```

```{r}
smaller_history <- smaller_model %>% 
  fit(x_train,
      y_train,
      epoch = 10,
      batch_size = 32,
      validation_split = 0.1,
      verbose = 2)
```

The smaller model is less accurate but showed improvement in loss. But we will go a step further and introduce a regularization using L2

```{r}
L2_model <- keras_model_sequential()
L2_model %>% 
  layer_dense(name = "DeepLayer1",
              units = 30,
              kernel_regularizer = regularizer_l2(0.01),
              activation = "relu",
              input_shape = c(30)) %>% 
  layer_dense(name = "DeepLayer2",
              units = 30, kernel_regularizer = regularizer_l2(0.01),
              activation = "relu") %>% 
  layer_dense(name = "OutputLayer",
              units = 1,
              activation = "sigmoid")
summary(L2_model)

L2_model %>% compile(loss = "binary_crossentropy",
                  optimizer = "rmsprop",
                  metrics = c("accuracy"))
```

```{r}
L2_history <- L2_model %>% 
  fit(x_train,
      y_train,
      epoch = 10,
      batch_size = 32,
      validation_split = 0.1,
      verbose = 2)

plot(L2_history)
```
The model is still overfit, so I attempted a dropout method.

```{r}
drop_model <- keras_model_sequential()
drop_model %>% 
  layer_dense(name = "DeepLayer1",
              units = 30,
              activation = "relu",
              input_shape = c(30)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(name = "DeepLayer2",
              units = 30,
              activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(name = "OutputLayer",
              units = 1,
              activation = "sigmoid")
summary(model)

drop_model %>% compile(loss = "binary_crossentropy",
                  optimizer = "rmsprop",
                  metrics = c("accuracy"))

```


```{r}
drop_history <- drop_model %>% 
  fit(x_train,
      y_train,
      epoch = 20,
      batch_size = 64,
      validation_split = 0.1,
      verbose = 2)

plot(drop_history)
```

```{r}
CancerFinal_reduced  <- read_csv("C:/Users/ianla.000/Downloads/data.csv")
CancerFinal_reduced <- CancerFinal_reduced[ -c(13:33) ]
CancerFinal_reduced 
```
```{r}
CancerFinal_reduced$NewDiagnosis<- ifelse(CancerFinal_reduced$diagnosis == "M", 0, 1) #recoded R column in new column and set M = 1 and B = 0

CancerFinal_reduced <- subset(CancerFinal_reduced, select = -c(id,diagnosis)) #used subsetting function to remove the original R column
CancerFinal_reduced


```

```{r}
table(CancerFinal_reduced$NewDiagnosis)
round(prop.table(table(CancerFinal_reduced$NewDiagnosis)) *100, digits =1)
```
```{r}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x))) } #normalized data using the technique taught in the lecture slides.
CancerNorm_reduced <- CancerFinal_reduced
CancerNorm_reduced <- as.data.frame(lapply(CancerNorm_reduced, normalize))
summary(CancerNorm_reduced)
```

```{r}
summary(CancerNorm_reduced)
```
```{r}

set.seed(123)
Cancer_train_reduced <- createDataPartition(y=CancerNorm_reduced$NewDiagnosis, p=0.75, list=FALSE)



x_train_reduced <- as.matrix(CancerNorm_reduced[Cancer_train_reduced, 1:10])
x_test_reduced <- as.matrix(CancerNorm_reduced[-Cancer_train_reduced, 1:10])

y_train_reduced <- as.matrix(CancerNorm_reduced[Cancer_train_reduced, 11])
y_test_reduced <- as.matrix(CancerNorm_reduced[-Cancer_train_reduced, 11])
```


```{r}
model_reduced <- keras_model_sequential()
model_reduced %>% 
  layer_dense(name = "DeepLayer1",
              units = 10,
              activation = "relu",
              input_shape = c(10)) %>% 
  layer_dense(name = "DeepLayer2",
              units = 10,
              activation = "relu") %>% 
  layer_dense(name = "DeepLayer3",
              units = 10,
              activation = "relu") %>%
  layer_dense(name = "OutputLayer",
              units = 1,
              activation = "sigmoid")
summary(model_reduced)


model_reduced %>% compile(loss = "binary_crossentropy",
                  optimizer = "rmsprop",
                  metrics = c("accuracy"))

history_reduced <- model_reduced %>% 
  fit(x_train,
      y_train,
      epoch = 10,
      batch_size = 64,
      validation_split = 0.1,
      verbose = 2)

```

